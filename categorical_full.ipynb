{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from itertools import chain\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from keras.metrics import top_k_categorical_accuracy\r\n",
    "from keras.models import Model\r\n",
    "from keras.layers import Dense, Dropout, Input, Embedding,Reshape, Concatenate, Conv1D, BatchNormalization, GlobalMaxPooling1D, MaxPooling1D\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device_name = tf.test.gpu_device_name()\r\n",
    "\r\n",
    "if device_name != '/device:GPU:0':\r\n",
    "\traise SystemError('GPU device not found')\r\n",
    "\r\n",
    "print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"./data/splited_full_RASFF_DATA.csv\", sep=\";\", header=0, index_col=0)\r\n",
    "data = data.sample(frac=1)\r\n",
    "\r\n",
    "data.head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Stage:\r\n",
    "\tdef __init__(self, input, output):\r\n",
    "\t\tself.input = input\r\n",
    "\t\tself.output = output\r\n",
    "\r\n",
    "\t\tself.x = data.iloc[:, input]\r\n",
    "\t\tself.y = data.iloc[:, output]\r\n",
    "\r\n",
    "\t\tself.x_train, self.y_train = None, None\r\n",
    "\t\tself.x_val, self.y_val = None, None\r\n",
    "\t\tself.x_test, self.y_test = None, None\r\n",
    "\r\n",
    "\t\tself.input_list_train, self.input_list_test, self.input_list_testval = None, None, None\r\n",
    "\r\n",
    "\t\tself.__transform()\r\n",
    "\r\n",
    "\tdef __transform(self):\r\n",
    "\t\tstrategy = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\r\n",
    "\t\tstrategy.fit(self.y.values)\r\n",
    "\r\n",
    "\t\tself.y = strategy.transform(self.y.values)\r\n",
    "\r\n",
    "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=0.2)\r\n",
    "\t\tself.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train, test_size=0.25, random_state=42,shuffle = True)\r\n",
    "\r\n",
    "\tdef get_metrics(self):\r\n",
    "\t\tresult = model.predict(self.input_list_test, batch_size=64)\r\n",
    "\t\tresult = np.argmax(result, axis=-1)\r\n",
    "\r\n",
    "\t\tprint(f\"- Accuracy: {round(accuracy_score(np.argmax(self.y_test, axis=-1), result)*100, 2)}%\")\r\n",
    "\t\tprint(f\"- Specifity: {round(get_specifity(np.argmax(self.y_test, axis=-1), result)*100, 2)}%\")\r\n",
    "\t\tprint(f\"- Sensitivity: {round(recall_score(np.argmax(self.y_test, axis=-1), result, average='macro', zero_division=0)*100, 2)}%\")\r\n",
    "\t\tprint(f\"- Precision: {round(precision_score(np.argmax(self.y_test, axis=-1), result, average='macro', zero_division=0)*100, 2)}%\")\r\n",
    "\r\n",
    "\t\tprint(classification_report(np.argmax(self.y_test, axis=-1), result, zero_division=True))\r\n",
    "\r\n",
    "\t\tcm = confusion_matrix(np.argmax(self.y_test, axis=-1), result)\r\n",
    "\t\tcm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(0, cm.shape[0])))\r\n",
    "\r\n",
    "\t\t_, ax = plt.subplots(figsize=(10, 10))\r\n",
    "\t\tcm.plot(ax=ax)\r\n",
    "\r\n",
    "\t\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.DATE_CASE = data.DATE_CASE.astype(str)\r\n",
    "data.HAZARDS_CAT = data.HAZARDS_CAT.astype(str)\r\n",
    "data.COUNT_DESTIN = data.COUNT_DESTIN.astype(str)\r\n",
    "data.COUNT_CONCERN = data.COUNT_CONCERN.astype(str)\r\n",
    "\r\n",
    "data = data.dropna(subset=['DATE_CASE'])\r\n",
    "\r\n",
    "# data.dropna(subset=data.columns[[1, 3, 5, 8, 9, 12, 13]], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preproc(X_train, X_test, X_val):\r\n",
    "    input_list_train = []\r\n",
    "    input_list_test = []\r\n",
    "    input_list_testval = []\r\n",
    "    \r\n",
    "    for c in stage1.x.columns:\r\n",
    "        raw_vals = np.unique(X_train[c])\r\n",
    "        val_map = {}\r\n",
    "        for i in range(len(raw_vals)):\r\n",
    "            val_map[raw_vals[i]] = i       \r\n",
    "        \r\n",
    "        input_list_train.append(X_train[c].map(val_map).values)\r\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\r\n",
    "        input_list_testval.append(X_val[c].map(val_map).fillna(0).values)\r\n",
    "\r\n",
    "    return input_list_train, input_list_test,input_list_testval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_specifity(y_actual, y_pred):\r\n",
    "    TN = []\r\n",
    "    FP = []\r\n",
    "\r\n",
    "    for index ,_id in enumerate(np.union1d(y_actual, y_pred)):\r\n",
    "        FP.append(0)\r\n",
    "        TN.append(0)\r\n",
    "\r\n",
    "        for i in range(len(y_pred)):\r\n",
    "            if y_pred[i] == _id and y_actual[i] != y_pred[i]:\r\n",
    "                FP[index] += 1\r\n",
    "            if y_actual[i] == y_pred[i] != _id:\r\n",
    "                TN[index] += 1\r\n",
    "\r\n",
    "    TN = sum(TN)\r\n",
    "    FP = sum(FP)\r\n",
    "\r\n",
    "    return TN/(TN + FP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chainer(s):\r\n",
    "    return list(chain.from_iterable(s.str.split(',')))\r\n",
    "\r\n",
    "lens = data['HAZARDS_CAT'].str.split(',').map(len)\r\n",
    "split1 = pd.DataFrame({'DATE_CASE': np.repeat(data['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(data['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(data['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(data['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(data['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(data['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(data['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': chainer(data['HAZARDS_CAT']),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(data['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(data['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(data['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split1['COUNT_ORIGEN'].str.split(',').map(len)\r\n",
    "split2 = pd.DataFrame({'DATE_CASE': np.repeat(split1['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split1['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split1['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split1['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split1['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split1['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split1['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split1['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': chainer(split1['COUNT_ORIGEN']),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split1['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split1['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split2['COUNT_DESTIN'].str.split(',').map(len)\r\n",
    "split3 = pd.DataFrame({'DATE_CASE': np.repeat(split2['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split2['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split2['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split2['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split2['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split2['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split2['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split2['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split2['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': chainer(split2['COUNT_DESTIN']),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split2['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split3['COUNT_CONCERN'].str.split(',').map(len)\r\n",
    "split4 = pd.DataFrame({'DATE_CASE': np.repeat(split3['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split3['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split3['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split3['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split3['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split3['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split3['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split3['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split3['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split3['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': chainer(split3['COUNT_CONCERN'])})\r\n",
    "\r\n",
    "split4 = split4.reset_index(drop = True)\r\n",
    "split4 = split4.dropna(subset = ['DATE_CASE'])\r\n",
    "\r\n",
    "data = split4.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Mining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stage1 = Stage(\r\n",
    "\tinput=[0, 1, 6, 8],\r\n",
    "\toutput=[2]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_1_categorical_accuracy(y_true, y_pred):\r\n",
    "\treturn top_k_categorical_accuracy(y_true, y_pred, k=1)\r\n",
    "\r\n",
    "def top_2_categorical_accuracy(y_true, y_pred):\r\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\r\n",
    "\r\n",
    "def top_3_categorical_accuracy(y_true, y_pred):\r\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_models = []\r\n",
    "output_embeddings = []\r\n",
    "\r\n",
    "for categorical_var in stage1.x.columns:\r\n",
    "    cat_emb_name = categorical_var.replace(\" \", \"\") + \"_Embedding\"\r\n",
    "    input_name = \"Input_\" + categorical_var.replace(\" \", \"\")\r\n",
    "    no_of_unique_cat = stage1.x_train[categorical_var].nunique()\r\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50))\r\n",
    "   \r\n",
    "    input_model = Input(shape=(1, ), name=input_name)\r\n",
    "    output_model = Embedding(no_of_unique_cat, embedding_size, name=cat_emb_name)(input_model)\r\n",
    "    output_model = Reshape(target_shape=(embedding_size, ))(output_model)    \r\n",
    "    \r\n",
    "    input_models.append(input_model)\r\n",
    "    output_embeddings.append(output_model)\r\n",
    "  \r\n",
    "output = Concatenate()(output_embeddings)\r\n",
    "output = Dense(2048,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.3)(output)\r\n",
    "output = Dense(1024,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(512,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(43, activation=\"softmax\")(output)\r\n",
    "\r\n",
    "model = Model(inputs=input_models, outputs=output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy,top_2_categorical_accuracy,top_3_categorical_accuracy])\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stage1.input_list_train, stage1.input_list_test, stage1.input_list_testval = preproc(stage1.x_train, stage1.x_test, stage1.x_val)\r\n",
    "\r\n",
    "hist = model.fit(stage1.input_list_train, stage1.y_train, validation_data=(stage1.input_list_testval, stage1.y_val), epochs=5, batch_size=64, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.style.use(\"ggplot\")\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "\r\n",
    "plt.plot(hist.history[\"loss\"], label=\"train_loss\")\r\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\r\n",
    "plt.plot(hist.history[\"accuracy\"], label=\"train_acc\")\r\n",
    "plt.plot(hist.history[\"val_accuracy\"], label=\"val_acc\")\r\n",
    "\r\n",
    "plt.title(\"Training Loss and Accuracy\")\r\n",
    "plt.xlabel(\"Epoch #\")\r\n",
    "plt.ylabel(\"Loss/Accuracy\")\r\n",
    "plt.legend(loc=\"lower left\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stage1.get_metrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historials = []\r\n",
    "evaluations = []\r\n",
    "\r\n",
    "for i in range (1, 6):\r\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy, top_2_categorical_accuracy, top_3_categorical_accuracy])\r\n",
    "    \r\n",
    "    hist = model.fit(stage1.input_list_train, stage1.y_train, validation_data=(stage1.input_list_testval, stage1.y_val), epochs=25, batch_size=64, verbose=0)\r\n",
    "    historials.append(hist)\r\n",
    "    \r\n",
    "    evaluation = model.evaluate(x=stage1.input_list_test, y=stage1.y_test)\r\n",
    "    evaluations.append(evaluation)\r\n",
    "    \r\n",
    "    model.save(\"model\" + str(i) + \".h5\")\r\n",
    "\r\n",
    "    stage1.get_metrics()\r\n",
    "\r\n",
    "    print(\"\\n\\n-----------------------\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_models = []\r\n",
    "output_embeddings = []\r\n",
    "\r\n",
    "for categorical_var in stage1.x.columns:\r\n",
    "\tcat_emb_name = categorical_var.replace(\" \", \"\") + \"_Embedding\"\r\n",
    "\tinput_name = \"Input_\" + categorical_var.replace(\" \", \"\")\r\n",
    "\tno_of_unique_cat  = stage1.x_train[categorical_var].nunique()\r\n",
    "\tembedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50))\r\n",
    "\r\n",
    "\tinput_model = Input(shape=(1, ), name=input_name)\r\n",
    "\toutput_model = Embedding(no_of_unique_cat, embedding_size, name=cat_emb_name)(input_model)\r\n",
    "\toutput_model = Reshape(target_shape=(embedding_size, ))(output_model)    \r\n",
    "\r\n",
    "\tinput_models.append(input_model)\r\n",
    "\toutput_embeddings.append(output_model)\r\n",
    "  \r\n",
    "output = Concatenate()(output_embeddings)\r\n",
    "output = Reshape(input_shape=(100,), target_shape=(100, 1))(output)\r\n",
    "output = Conv1D(filters=128,kernel_size=4, activation = \"relu\")(output)\r\n",
    "output = Conv1D(filters=128,kernel_size=4, activation = \"relu\")(output)\r\n",
    "output = BatchNormalization()(output)\r\n",
    "output = MaxPooling1D(pool_size=2)(output)\r\n",
    "output = Conv1D(filters=256,kernel_size=3, activation = \"relu\")(output)\r\n",
    "output = Conv1D(filters=256,kernel_size=3, activation = \"relu\")(output)\r\n",
    "output = BatchNormalization()(output)\r\n",
    "output = GlobalMaxPooling1D()(output)\r\n",
    "output = Dense(512, activation = \"relu\")(output)\r\n",
    "output = Dense(256, activation = \"relu\")(output)\r\n",
    "output = Dense(43, activation='softmax')(output)\r\n",
    "\r\n",
    "model = Model(inputs=input_models, outputs=output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[top_1_categorical_accuracy,top_2_categorical_accuracy,top_3_categorical_accuracy])\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stage1.input_list_train, stage1.input_list_test, stage1.input_list_testval = preproc(stage1.x_train, stage1.x_test, stage1.x_val)\r\n",
    "\r\n",
    "hist = model.fit(stage1.input_list_train, stage1.y_train, validation_data=(stage1.input_list_testval, stage1.y_val) , epochs =  25, batch_size = 64, verbose= 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate(x=stage1.input_list_test, y=stage1.y_test)\r\n",
    "\r\n",
    "stage1.get_metrics()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ceiec': conda)"
  },
  "interpreter": {
   "hash": "d98ed641ffb0afae769cd9f96943c7d5fe6ff4d949b5ee86e4a8c8a547fca37e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}