{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from keras import metrics\r\n",
    "from keras.models import Model\r\n",
    "from keras.layers import Dense, Dropout, Input, Embedding,Reshape, Concatenate\r\n",
    "from itertools import chain"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data = pd.read_csv(\"../data/full_RASFF_DATA.csv\", sep=\";\", header=0, index_col=0)\r\n",
    "data = data.sample(frac=1)\r\n",
    "\r\n",
    "data.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASSIF</th>\n",
       "      <th>DATE_CASE</th>\n",
       "      <th>REF</th>\n",
       "      <th>NOT_COUNTRY</th>\n",
       "      <th>SUBJET</th>\n",
       "      <th>PROD_CAT</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>RISK_DECISION</th>\n",
       "      <th>ACTION_TAKEN</th>\n",
       "      <th>DISTRIBUTION_STAT</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>HAZARDS</th>\n",
       "      <th>HAZARDS_CAT</th>\n",
       "      <th>COUNT_ORIGEN</th>\n",
       "      <th>COUNT_DESTIN</th>\n",
       "      <th>COUNT_CONCERN</th>\n",
       "      <th>NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>alert</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2020.1405</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Listeria monocytogenes (presence /25g) in froz...</td>\n",
       "      <td>ices and desserts</td>\n",
       "      <td>food</td>\n",
       "      <td>serious</td>\n",
       "      <td>seizure</td>\n",
       "      <td>distribution to other member countries</td>\n",
       "      <td>frozen bavarois pastry</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>microbial contaminants (other)</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Germany,Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41862</th>\n",
       "      <td>border rejection</td>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>2008.ANI</td>\n",
       "      <td>Italy</td>\n",
       "      <td>migration of nickel (0.81; 1.75 mg/l) from cut...</td>\n",
       "      <td>food contact materials</td>\n",
       "      <td>fcm</td>\n",
       "      <td>undecided</td>\n",
       "      <td>re-dispatch</td>\n",
       "      <td>no distribution</td>\n",
       "      <td>cutlery set</td>\n",
       "      <td>migration of nickel</td>\n",
       "      <td>metals</td>\n",
       "      <td>India</td>\n",
       "      <td></td>\n",
       "      <td>Italy</td>\n",
       "      <td>38280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>information for follow-up</td>\n",
       "      <td>2015-03-02</td>\n",
       "      <td>2015.0240</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>unauthorised placing on the market of salmon f...</td>\n",
       "      <td>fish and fish products</td>\n",
       "      <td>food</td>\n",
       "      <td>undecided</td>\n",
       "      <td>informing authorities</td>\n",
       "      <td>distribution to other member countries</td>\n",
       "      <td>salmon</td>\n",
       "      <td>unauthorised placing on the market</td>\n",
       "      <td>adulteration / fraud</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Belgium,Denmark,France,Sweden</td>\n",
       "      <td>Latvia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22392</th>\n",
       "      <td>alert</td>\n",
       "      <td>2014-02-20</td>\n",
       "      <td>2014.0249</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>migration of benzophenone (4.7; 6.7 mg/kg - pp...</td>\n",
       "      <td>food contact materials</td>\n",
       "      <td>fcm</td>\n",
       "      <td>undecided</td>\n",
       "      <td>withdrawal from the market</td>\n",
       "      <td>distribution to other member countries</td>\n",
       "      <td>plastic pack containing gluten free muesli and...</td>\n",
       "      <td>migration of benzophenone</td>\n",
       "      <td>migration</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Ireland,Malta,United Kingdom</td>\n",
       "      <td>Commission Services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24324</th>\n",
       "      <td>border rejection</td>\n",
       "      <td>2013-06-27</td>\n",
       "      <td>2013.BEZ</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Salmonella enterica (presence /25g) in frozen ...</td>\n",
       "      <td>poultry meat and poultry meat products</td>\n",
       "      <td>food</td>\n",
       "      <td>not serious</td>\n",
       "      <td></td>\n",
       "      <td>product not (yet) placed on the market</td>\n",
       "      <td>frozen chicken</td>\n",
       "      <td>salmonella</td>\n",
       "      <td>pathogenic micro-organisms</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         CLASSIF   DATE_CASE        REF NOT_COUNTRY  \\\n",
       "1720                       alert  2020-03-26  2020.1405     Belgium   \n",
       "41862           border rejection  2008-03-28   2008.ANI       Italy   \n",
       "19290  information for follow-up  2015-03-02  2015.0240      Sweden   \n",
       "22392                      alert  2014-02-20  2014.0249     Ireland   \n",
       "24324           border rejection  2013-06-27   2013.BEZ       Spain   \n",
       "\n",
       "                                                  SUBJET  \\\n",
       "1720   Listeria monocytogenes (presence /25g) in froz...   \n",
       "41862  migration of nickel (0.81; 1.75 mg/l) from cut...   \n",
       "19290  unauthorised placing on the market of salmon f...   \n",
       "22392  migration of benzophenone (4.7; 6.7 mg/kg - pp...   \n",
       "24324  Salmonella enterica (presence /25g) in frozen ...   \n",
       "\n",
       "                                     PROD_CAT  TYPE RISK_DECISION  \\\n",
       "1720                        ices and desserts  food       serious   \n",
       "41862                  food contact materials   fcm     undecided   \n",
       "19290                  fish and fish products  food     undecided   \n",
       "22392                  food contact materials   fcm     undecided   \n",
       "24324  poultry meat and poultry meat products  food   not serious   \n",
       "\n",
       "                     ACTION_TAKEN                       DISTRIBUTION_STAT  \\\n",
       "1720                      seizure  distribution to other member countries   \n",
       "41862                 re-dispatch                         no distribution   \n",
       "19290       informing authorities  distribution to other member countries   \n",
       "22392  withdrawal from the market  distribution to other member countries   \n",
       "24324                              product not (yet) placed on the market   \n",
       "\n",
       "                                                 PRODUCT  \\\n",
       "1720                              frozen bavarois pastry   \n",
       "41862                                        cutlery set   \n",
       "19290                                             salmon   \n",
       "22392  plastic pack containing gluten free muesli and...   \n",
       "24324                                     frozen chicken   \n",
       "\n",
       "                                  HAZARDS                     HAZARDS_CAT  \\\n",
       "1720               listeria monocytogenes  microbial contaminants (other)   \n",
       "41862                 migration of nickel                          metals   \n",
       "19290  unauthorised placing on the market            adulteration / fraud   \n",
       "22392           migration of benzophenone                       migration   \n",
       "24324                          salmonella      pathogenic micro-organisms   \n",
       "\n",
       "      COUNT_ORIGEN                   COUNT_DESTIN        COUNT_CONCERN  \\\n",
       "1720       Belgium            Germany,Netherlands                  NaN   \n",
       "41862        India                                               Italy   \n",
       "19290       Sweden  Belgium,Denmark,France,Sweden               Latvia   \n",
       "22392      Ireland   Ireland,Malta,United Kingdom  Commission Services   \n",
       "24324       Brazil                            NaN                Spain   \n",
       "\n",
       "        NUMBER  \n",
       "1720       NaN  \n",
       "41862  38280.0  \n",
       "19290      NaN  \n",
       "22392      NaN  \n",
       "24324      NaN  "
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"Initial length:\", len(data))\r\n",
    "\r\n",
    "data.HAZARDS_CAT = data.HAZARDS_CAT.astype(str)\r\n",
    "data.DATE_CASE = data.DATE_CASE.astype(str)\r\n",
    "data.DATE_CASE = pd.to_datetime(data.DATE_CASE, errors=\"coerce\")\r\n",
    "data.DATE_CASE = data.DATE_CASE.dt.month\r\n",
    "\r\n",
    "data.dropna(subset=[\"DATE_CASE\"], inplace=True)\r\n",
    "\r\n",
    "print(\"Final length:\", len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial length: 60066\n",
      "Final length: 60065\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def chainer(s):\r\n",
    "    return list(chain.from_iterable(s.str.split(',')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "lens = data['HAZARDS_CAT'].str.split(',').map(len)\r\n",
    "split1 = pd.DataFrame({'DATE_CASE': np.repeat(data['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(data['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(data['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(data['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(data['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(data['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(data['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': chainer(data['HAZARDS_CAT']),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(data['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(data['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(data['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split1['COUNT_ORIGEN'].str.split(',').map(len)\r\n",
    "split2 = pd.DataFrame({'DATE_CASE': np.repeat(split1['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split1['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split1['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split1['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split1['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split1['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split1['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split1['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': chainer(split1['COUNT_ORIGEN']),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split1['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split1['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split2['COUNT_DESTIN'].str.split(',').map(len)\r\n",
    "split3 = pd.DataFrame({'DATE_CASE': np.repeat(split2['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split2['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split2['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split2['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split2['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split2['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split2['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split2['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split2['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': chainer(split2['COUNT_DESTIN']),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split2['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split3['COUNT_CONCERN'].str.split(',').map(len)\r\n",
    "split4 = pd.DataFrame({'DATE_CASE': np.repeat(split3['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split3['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split3['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split3['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split3['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split3['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split3['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split3['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split3['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split3['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': chainer(split3['COUNT_CONCERN'])})\r\n",
    "\r\n",
    "split4 = split4.reset_index(drop = True)\r\n",
    "split4 = split4.dropna(subset = ['DATE_CASE'])\r\n",
    "\r\n",
    "data = split4.copy()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20056/839995518.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                     'COUNT_CONCERN': np.repeat(data['COUNT_CONCERN'], lens)})\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mlens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'COUNT_ORIGEN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m split2 = pd.DataFrame({'DATE_CASE': np.repeat(split1['DATE_CASE'], lens),\n\u001b[0;32m     16\u001b[0m                     \u001b[1;34m'NOT_COUNTRY'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOT_COUNTRY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\ceiec\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m         \"\"\"\n\u001b[1;32m-> 4161\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\ceiec\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\ceiec\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_vars = [0, 1, 6, 8]\r\n",
    "target_vars = [2]\r\n",
    "\r\n",
    "X = data.iloc[:, categorical_vars]\r\n",
    "Y = data.iloc[:, target_vars]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ency = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\r\n",
    "\r\n",
    "ency.fit(Y.values)\r\n",
    "\r\n",
    "y_one_hot = ency.transform(Y.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split train-val-test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(X, y_one_hot, test_size=0.2, random_state=42, shuffle=True)\r\n",
    "x_training_data, x_val_data, y_training_data, y_val_data = train_test_split(x_training_data, y_training_data, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coding and conversion to lists for beign able to introduce it into the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_vars = data.iloc[:, categorical_vars].columns\r\n",
    "\r\n",
    "def preproc(X_train, X_test, X_val):\r\n",
    "    input_list_train = []\r\n",
    "    input_list_test = []\r\n",
    "    input_list_testval = []\r\n",
    "    \r\n",
    "    for c in categorical_vars:\r\n",
    "        raw_vals = np.unique(X_train[c])\r\n",
    "        val_map = {}\r\n",
    "        for i in range(len(raw_vals)):\r\n",
    "            val_map[raw_vals[i]] = i       \r\n",
    "        \r\n",
    "        input_list_train.append(X_train[c].map(val_map).values)\r\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\r\n",
    "        input_list_testval.append(X_val[c].map(val_map).fillna(0).values)\r\n",
    "\r\n",
    "    return input_list_train, input_list_test,input_list_testval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_list_train, input_list_test, input_list_testval = preproc(x_training_data, x_test_data, x_val_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_1_categorical_accuracy(y_true, y_pred):\r\n",
    "\treturn metrics.top_k_categorical_accuracy(y_true, y_pred, k=1)\r\n",
    "\r\n",
    "def top_2_categorical_accuracy(y_true, y_pred):\r\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=2)\r\n",
    "\r\n",
    "def top_3_categorical_accuracy(y_true, y_pred):\r\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings + MLP Models (cases 1 and 3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_models = []\r\n",
    "output_embeddings = []\r\n",
    "\r\n",
    "for categorical_var in categorical_vars:\r\n",
    "    cat_emb_name = categorical_var.replace(\" \", \"\") + \"_Embedding\"\r\n",
    "    input_name = \"Input_\" + categorical_var.replace(\" \", \"\")\r\n",
    "    no_of_unique_cat = x_training_data[categorical_var].nunique()\r\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50))\r\n",
    "   \r\n",
    "    input_model = Input(shape=(1, ), name=input_name)\r\n",
    "    output_model = Embedding(no_of_unique_cat, embedding_size, name=cat_emb_name)(input_model)\r\n",
    "    output_model = Reshape(target_shape=(embedding_size, ))(output_model)    \r\n",
    "    \r\n",
    "    input_models.append(input_model)\r\n",
    "    output_embeddings.append(output_model)\r\n",
    "  \r\n",
    "output = Concatenate()(output_embeddings)\r\n",
    "output = Dense(2048,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.3)(output)\r\n",
    "output = Dense(1024,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(512,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(42, activation=\"softmax\")(output)\r\n",
    "\r\n",
    "model = Model(inputs=input_models, outputs=output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy,top_2_categorical_accuracy,top_3_categorical_accuracy])\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# REVIEW: No validation data has been provided\r\n",
    " \r\n",
    "# hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_test, y_test_data), epochs=5 , batch_size=64, verbose=1)\r\n",
    "hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_testval, y_val_data), epochs=5, batch_size=64, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_metrics():\r\n",
    "\tresult = model.predict(input_list_test, batch_size=64)\r\n",
    "\tresult = np.argmax(result, axis=-1)\r\n",
    "\r\n",
    "\tvalid_loss, valid_accuracy, acc1, acc2, acc3 = model.evaluate(input_list_test)\r\n",
    "\r\n",
    "\tprint(\"Loss:\", valid_loss)\r\n",
    "\tprint(\"Accuracy:\", valid_accuracy)\r\n",
    "\tprint(\"Top-1 Accuracy:\", acc1)\r\n",
    "\tprint(\"Top-2 Accuracy:\", acc2)\r\n",
    "\tprint(\"Top-3 Accuracy:\", acc3)\r\n",
    "\r\n",
    "\tprint(classification_report(np.argmax(y_test_data, axis=-1), result, zero_division=True))\r\n",
    "\r\n",
    "\tcm = confusion_matrix(np.argmax(y_test_data, axis=-1), result)\r\n",
    "\tcm = ConfusionMatrixDisplay(confusion_matrix=cm)\r\n",
    "\r\n",
    "\tfig, ax = plt.subplots(figsize=(20, 20))\r\n",
    "\tcm.plot(ax=ax)\r\n",
    "\r\n",
    "\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_metrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = model.predict(input_list_test, batch_size=64)\r\n",
    "# result = np.argmax(result, axis=1)\r\n",
    "\r\n",
    "# valid_loss, valid_accuracy, acc1, acc2, acc3 = model.evaluate(input_list_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"Loss:\", valid_loss)\r\n",
    "# print(\"Accuracy:\", valid_accuracy)\r\n",
    "# print(\"Top-1 Accuracy:\", acc1)\r\n",
    "# print(\"Top-2 Accuracy:\", acc2)\r\n",
    "# print(\"Top-3 Accuracy:\", acc3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(classification_report(np.argmax(y_test_data, axis=-1), result, zero_division=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cm = confusion_matrix(np.argmax(y_test_data, axis=-1), result)\r\n",
    "# cm = ConfusionMatrixDisplay(confusion_matrix=cm)\r\n",
    "\r\n",
    "# fig, ax = plt.subplots(figsize=(20, 20))\r\n",
    "# cm.plot(ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 5\r\n",
    "\r\n",
    "plt.style.use(\"ggplot\")\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"loss\"], label=\"train_loss\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"val_loss\"], label=\"val_loss\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"accuracy\"], label=\"train_acc\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"val_accuracy\"], label=\"val_acc\")\r\n",
    "\r\n",
    "plt.title(\"Training Loss and Accuracy\")\r\n",
    "plt.xlabel(\"Epoch #\")\r\n",
    "plt.ylabel(\"Loss/Accuracy\")\r\n",
    "plt.legend(loc=\"lower left\")\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historials = []\r\n",
    "evaluations = []\r\n",
    "\r\n",
    "for i in range (1, 6):\r\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy, top_2_categorical_accuracy, top_3_categorical_accuracy])\r\n",
    "    \r\n",
    "    hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_testval, y_val_data), epochs=25, batch_size=64, verbose=0)\r\n",
    "    historials.append(hist)\r\n",
    "    \r\n",
    "    evaluation = model.evaluate(x=input_list_test, y=y_test_data)\r\n",
    "    evaluations.append(evaluation)\r\n",
    "    \r\n",
    "    model.save(\"model\" + str(i) + \".h5\")\r\n",
    "\r\n",
    "    get_metrics()\r\n",
    "\r\n",
    "    print(\"\\n\\n-----------------------\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ceiec': conda)"
  },
  "interpreter": {
   "hash": "d98ed641ffb0afae769cd9f96943c7d5fe6ff4d949b5ee86e4a8c8a547fca37e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}