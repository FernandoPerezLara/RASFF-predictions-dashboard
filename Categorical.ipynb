{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from keras import metrics\r\n",
    "from keras.models import Model\r\n",
    "from keras.layers import Dense, Dropout, Input, Embedding,Reshape, Concatenate\r\n",
    "from itertools import chain"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device_name = tf.test.gpu_device_name()\r\n",
    "\r\n",
    "if device_name != '/device:GPU:0':\r\n",
    "\traise SystemError('GPU device not found')\r\n",
    "\r\n",
    "print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26692/1199953204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"./data/full_RASFF_DATA.csv\", sep=\";\", header=0, index_col=0)\r\n",
    "\r\n",
    "data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.DATE_CASE = data.DATE_CASE.astype(str)\r\n",
    "data.HAZARDS_CAT = data.HAZARDS_CAT.astype(str)\r\n",
    "data.COUNT_DESTIN = data.COUNT_DESTIN.astype(str)\r\n",
    "data.COUNT_CONCERN = data.COUNT_CONCERN.astype(str)\r\n",
    "\r\n",
    "data.dropna(subset=data.columns[[1, 3, 5, 8, 9, 12, 13]], inplace=True)\r\n",
    "\r\n",
    "data = data.sample(frac=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chainer(s):\r\n",
    "    return list(chain.from_iterable(s.str.split(',')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lens = data['HAZARDS_CAT'].str.split(',').map(len)\r\n",
    "split1 = pd.DataFrame({'DATE_CASE': np.repeat(data['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(data['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(data['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(data['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(data['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(data['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(data['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': chainer(data['HAZARDS_CAT']),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(data['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(data['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(data['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split1['COUNT_ORIGEN'].str.split(',').map(len)\r\n",
    "split2 = pd.DataFrame({'DATE_CASE': np.repeat(split1['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split1['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split1['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split1['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split1['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split1['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split1['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split1['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': chainer(split1['COUNT_ORIGEN']),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split1['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split1['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split2['COUNT_DESTIN'].str.split(',').map(len)\r\n",
    "split3 = pd.DataFrame({'DATE_CASE': np.repeat(split2['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split2['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split2['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split2['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split2['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split2['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split2['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split2['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split2['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': chainer(split2['COUNT_DESTIN']),\r\n",
    "                    'COUNT_CONCERN': np.repeat(split2['COUNT_CONCERN'], lens)})\r\n",
    "\r\n",
    "lens = split3['COUNT_CONCERN'].str.split(',').map(len)\r\n",
    "split4 = pd.DataFrame({'DATE_CASE': np.repeat(split3['DATE_CASE'], lens),\r\n",
    "                    'NOT_COUNTRY': np.repeat(split3['NOT_COUNTRY'], lens),\r\n",
    "                    'PROD_CAT': np.repeat(split3['PROD_CAT'], lens),\r\n",
    "                    'TYPE': np.repeat(split3['TYPE'], lens),\r\n",
    "                    'RISK_DECISION': np.repeat(split3['RISK_DECISION'], lens),\r\n",
    "                    'ACTION_TAKEN': np.repeat(split3['ACTION_TAKEN'], lens),\r\n",
    "                    'DISTRIBUTION_STAT': np.repeat(split3['DISTRIBUTION_STAT'], lens),\r\n",
    "                    'HAZARDS_CAT': np.repeat(split3['HAZARDS_CAT'], lens),\r\n",
    "                    'COUNT_ORIGEN': np.repeat(split3['COUNT_ORIGEN'], lens),\r\n",
    "                    'COUNT_DESTIN': np.repeat(split3['COUNT_DESTIN'], lens),\r\n",
    "                    'COUNT_CONCERN': chainer(split3['COUNT_CONCERN'])})\r\n",
    "\r\n",
    "split4 = split4.reset_index(drop = True)\r\n",
    "split4 = split4.dropna(subset = ['DATE_CASE'])\r\n",
    "\r\n",
    "data = split4.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_vars = [0, 1, 6, 8]\r\n",
    "target_vars = [2]\r\n",
    "\r\n",
    "X = data.iloc[:, categorical_vars]\r\n",
    "Y = data.iloc[:, target_vars]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ency = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\r\n",
    "\r\n",
    "ency.fit(Y.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split train-val-test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(X, y_one_hot, test_size=0.2, random_state=42, shuffle=True)\r\n",
    "\r\n",
    "train_mask = (data.DATE_CASE >= \"2004-01-01\") & (data.DATE_CASE <= \"2018-12-31\")\r\n",
    "test_mask = (data.DATE_CASE >= \"2019-01-01\") & (data.DATE_CASE <= \"2019-12-31\")\r\n",
    "\r\n",
    "x_training_data = X.loc[train_mask]\r\n",
    "y_training_data = Y.loc[train_mask]\r\n",
    "x_test_data = X.loc[test_mask]\r\n",
    "y_test_data = Y.loc[test_mask]\r\n",
    "\r\n",
    "x_training_data, x_val_data, y_training_data, y_val_data = train_test_split(x_training_data, y_training_data, test_size=0.2, random_state=42, shuffle=True)\r\n",
    "\r\n",
    "x_training_data = ency.transform(x_training_data.values)\r\n",
    "x_test_data = ency.transform(x_test_data.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coding and conversion to lists for beign able to introduce it into the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_vars = data.iloc[:, categorical_vars].columns\r\n",
    "\r\n",
    "def preproc(X_train, X_test, X_val):\r\n",
    "    input_list_train = []\r\n",
    "    input_list_test = []\r\n",
    "    input_list_testval = []\r\n",
    "    \r\n",
    "    for c in categorical_vars:\r\n",
    "        raw_vals = np.unique(X_train[c])\r\n",
    "        val_map = {}\r\n",
    "        for i in range(len(raw_vals)):\r\n",
    "            val_map[raw_vals[i]] = i       \r\n",
    "        \r\n",
    "        input_list_train.append(X_train[c].map(val_map).values)\r\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\r\n",
    "        input_list_testval.append(X_val[c].map(val_map).fillna(0).values)\r\n",
    "\r\n",
    "    return input_list_train, input_list_test,input_list_testval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_list_train, input_list_test, input_list_testval = preproc(x_training_data, x_test_data, x_val_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_1_categorical_accuracy(y_true, y_pred):\r\n",
    "\treturn metrics.top_k_categorical_accuracy(y_true, y_pred, k=1)\r\n",
    "\r\n",
    "def top_2_categorical_accuracy(y_true, y_pred):\r\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=2)\r\n",
    "\r\n",
    "def top_3_categorical_accuracy(y_true, y_pred):\r\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings + MLP Models (cases 1 and 3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_models = []\r\n",
    "output_embeddings = []\r\n",
    "\r\n",
    "for categorical_var in categorical_vars:\r\n",
    "    cat_emb_name = categorical_var.replace(\" \", \"\") + \"_Embedding\"\r\n",
    "    input_name = \"Input_\" + categorical_var.replace(\" \", \"\")\r\n",
    "    no_of_unique_cat = x_training_data[categorical_var].nunique()\r\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50))\r\n",
    "   \r\n",
    "    input_model = Input(shape=(1, ), name=input_name)\r\n",
    "    output_model = Embedding(no_of_unique_cat, embedding_size, name=cat_emb_name)(input_model)\r\n",
    "    output_model = Reshape(target_shape=(embedding_size, ))(output_model)    \r\n",
    "    \r\n",
    "    input_models.append(input_model)\r\n",
    "    output_embeddings.append(output_model)\r\n",
    "  \r\n",
    "output = Concatenate()(output_embeddings)\r\n",
    "output = Dense(2048,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.3)(output)\r\n",
    "output = Dense(1024,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(512,activation=\"relu\")(output)\r\n",
    "output = Dropout(0.2)(output)\r\n",
    "output = Dense(42, activation=\"softmax\")(output)\r\n",
    "\r\n",
    "model = Model(inputs=input_models, outputs=output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy,top_2_categorical_accuracy,top_3_categorical_accuracy])\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# REVIEW: No validation data has been provided\r\n",
    " \r\n",
    "# hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_test, y_test_data), epochs=5 , batch_size=64, verbose=1)\r\n",
    "hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_testval, y_val_data), epochs=5, batch_size=64, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_specifity(y_actual, y_pred):\r\n",
    "    TN = []\r\n",
    "    FP = []\r\n",
    "\r\n",
    "    for index ,_id in enumerate(np.union1d(y_actual, y_pred)):\r\n",
    "        FP.append(0)\r\n",
    "        TN.append(0)\r\n",
    "\r\n",
    "        for i in range(len(y_pred)):\r\n",
    "            if y_pred[i] == _id and y_actual[i] != y_pred[i]:\r\n",
    "                FP[index] += 1\r\n",
    "            if y_actual[i] == y_pred[i] != _id:\r\n",
    "                TN[index] += 1\r\n",
    "\r\n",
    "    TN = sum(TN)\r\n",
    "    FP = sum(FP)\r\n",
    "\r\n",
    "    return TN/(TN + FP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_metrics():\r\n",
    "\tresult = model.predict(input_list_test, batch_size=64)\r\n",
    "\tresult = np.argmax(result, axis=-1)\r\n",
    "\r\n",
    "\tvalid_loss, valid_accuracy, acc1, acc2, acc3 = model.evaluate(input_list_test)\r\n",
    "\r\n",
    "\tprint(\"Loss:\", valid_loss)\r\n",
    "\tprint(\"Accuracy:\", valid_accuracy)\r\n",
    "\tprint(\"Top-1 Accuracy:\", acc1)\r\n",
    "\tprint(\"Top-2 Accuracy:\", acc2)\r\n",
    "\tprint(\"Top-3 Accuracy:\", acc3)\r\n",
    "\tprint(f\"Specifity: {round(get_specifity(input_list_test, result)*100, 2)}%\")\r\n",
    "\tprint(f\"Sensitivity: {round(recall_score(input_list_test, result.y_predict, average='macro', zero_division=0)*100, 2)}%\")\r\n",
    "\tprint(f\"Precision: {round(precision_score(input_list_test, result, average='macro', zero_division=0)*100, 2)}%\")\r\n",
    "\r\n",
    "\tprint(classification_report(np.argmax(y_test_data, axis=-1), result, zero_division=True))\r\n",
    "\r\n",
    "\tcm = confusion_matrix(np.argmax(y_test_data, axis=-1), result)\r\n",
    "\tcm = ConfusionMatrixDisplay(confusion_matrix=cm)\r\n",
    "\r\n",
    "\tfig, ax = plt.subplots(figsize=(20, 20))\r\n",
    "\tcm.plot(ax=ax)\r\n",
    "\r\n",
    "\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_metrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = model.predict(input_list_test, batch_size=64)\r\n",
    "# result = np.argmax(result, axis=1)\r\n",
    "\r\n",
    "# valid_loss, valid_accuracy, acc1, acc2, acc3 = model.evaluate(input_list_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"Loss:\", valid_loss)\r\n",
    "# print(\"Accuracy:\", valid_accuracy)\r\n",
    "# print(\"Top-1 Accuracy:\", acc1)\r\n",
    "# print(\"Top-2 Accuracy:\", acc2)\r\n",
    "# print(\"Top-3 Accuracy:\", acc3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(classification_report(np.argmax(y_test_data, axis=-1), result, zero_division=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cm = confusion_matrix(np.argmax(y_test_data, axis=-1), result)\r\n",
    "# cm = ConfusionMatrixDisplay(confusion_matrix=cm)\r\n",
    "\r\n",
    "# fig, ax = plt.subplots(figsize=(20, 20))\r\n",
    "# cm.plot(ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 5\r\n",
    "\r\n",
    "plt.style.use(\"ggplot\")\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"loss\"], label=\"train_loss\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"val_loss\"], label=\"val_loss\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"accuracy\"], label=\"train_acc\")\r\n",
    "plt.plot(np.arange(0, N), hist.history[\"val_accuracy\"], label=\"val_acc\")\r\n",
    "\r\n",
    "plt.title(\"Training Loss and Accuracy\")\r\n",
    "plt.xlabel(\"Epoch #\")\r\n",
    "plt.ylabel(\"Loss/Accuracy\")\r\n",
    "plt.legend(loc=\"lower left\")\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historials = []\r\n",
    "evaluations = []\r\n",
    "\r\n",
    "for i in range (1, 6):\r\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", top_1_categorical_accuracy, top_2_categorical_accuracy, top_3_categorical_accuracy])\r\n",
    "    \r\n",
    "    hist = model.fit(input_list_train, y_training_data, validation_data=(input_list_testval, y_val_data), epochs=25, batch_size=64, verbose=0)\r\n",
    "    historials.append(hist)\r\n",
    "    \r\n",
    "    evaluation = model.evaluate(x=input_list_test, y=y_test_data)\r\n",
    "    evaluations.append(evaluation)\r\n",
    "    \r\n",
    "    model.save(\"model\" + str(i) + \".h5\")\r\n",
    "\r\n",
    "    get_metrics()\r\n",
    "\r\n",
    "    print(\"\\n\\n-----------------------\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ceiec': conda)"
  },
  "interpreter": {
   "hash": "d98ed641ffb0afae769cd9f96943c7d5fe6ff4d949b5ee86e4a8c8a547fca37e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}